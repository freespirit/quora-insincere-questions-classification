{
  "cells": [
    {
      "metadata": {
        "_uuid": "2d3a0ab217b4ff479c26eec9815f02ec202732cd"
      },
      "cell_type": "markdown",
      "source": "# Preprocessing experiments\n\nInspired by https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings , check what's the situtatoin with the current approach - Keras Tokenizer + Embedding constants (e.g. MAX_WORDS)"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import keras\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import metrics\n\nfrom collections import defaultdict\nimport operator\nimport re\nfrom tqdm import tqdm\n\n\nprint(os.listdir(\"../input\"))",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "['embeddings', 'train.csv', 'sample_submission.csv', 'test.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "MAX_SEQUENCE_LENGTH = 60\nMAX_WORDS = 95000\nEMBEDDINGS_LOADED_DIMENSIONS = 300",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "80c188d28dac370261b2fba477f6f85f58756a94"
      },
      "cell_type": "code",
      "source": "df_train = pd.read_csv(\"../input/train.csv\")\ndf_test = pd.read_csv(\"../input/test.csv\")",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1485d03a810b6f433541f7cf8ab373be895bd139"
      },
      "cell_type": "code",
      "source": "BATCH_SIZE = 256\nQ_FRACTION = 1\nquestions = df_train.sample(frac=Q_FRACTION)\nquestion_texts = questions[\"question_text\"].values\nquestion_targets = questions[\"target\"].values\ntest_texts = df_test[\"question_text\"].fillna(\"_na_\").values\n\nprint(f\"Working on {len(questions)} questions\")",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Working on 1306122 questions\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "adc1bbdf0874d110e69a484191ad5becc34e09bc"
      },
      "cell_type": "code",
      "source": "def load_embeddings(file):\n    embeddings = {}\n    with open(file, encoding=\"utf8\", errors='ignore') as f:\n        def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n        embeddings = dict(get_coefs(*line.split(\" \")) for line in f)\n        \n    print('Found %s word vectors.' % len(embeddings))\n    return embeddings",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dac5ea1dbf060a2618c9bab850e6daabd4473ba5"
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words=MAX_WORDS)\ntokenizer.fit_on_texts(list(df_train[\"question_text\"].values))",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ec53ddc5c350bbd7b614d6480c7bdd42135ea63d"
      },
      "cell_type": "markdown",
      "source": "# Preprocessing\n\nDefine a Preprocessor class based on https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings\nIt's future goal would be to contribute to a known model and explore any improvement in the final score"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f74efdf7fab99646bcdb6faa5619cb03778e191c"
      },
      "cell_type": "code",
      "source": "class Preprocessor:\n    def __init__(self, embeddings: dict):\n        self.embeddings = embeddings\n\n    def build_tf_dict(self, sentences: list):\n        \"\"\"\n        Build a simple TF (term frequency) dictionary for all words in the provided sentences.\n        \"\"\"\n        tf_dict = defaultdict(int)\n        for sentence in sentences:\n            for word in sentence:\n                tf_dict[word] += 1\n        return tf_dict\n\n    def check_coverage(self, tf_dictionary: dict):\n        \"\"\"\n        Build a simple list of words that are not embedded. Can be used down the stream to preprocess them to something\n        known.\n        \"\"\"\n        in_vocabulary = defaultdict(int)\n        out_of_vocabulary = defaultdict(int)\n        in_count = 0\n        out_count = 0\n\n        for word in tqdm(tf_dictionary):\n            if word in self.embeddings:\n                in_vocabulary[word] = self.embeddings[word]\n                in_count += tf_dictionary[word]\n            else:\n                out_of_vocabulary[word] = tf_dictionary[word]\n                out_count += tf_dictionary[word]\n\n        percent_tf = len(in_vocabulary) / len(tf_dictionary)\n        percent_all = in_count / (in_count + out_count)\n        print('Found embeddings for {:.2%} of vocabulary and {:.2%} of all text'.format(percent_tf, percent_all))\n\n        return sorted(out_of_vocabulary.items(), key=operator.itemgetter(1))[::-1]\n\n    def clean_punctuation(self, text: list):\n        result = text\n        \n        for punct in \"/-'\":\n            result = result.replace(punct, ' ')\n        for punct in '&':\n            result = result.replace(punct, f' {punct} ')\n        for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n            result = result.replace(punct, '')\n\n        return result\n\n    def clean_digits(self, text: list):\n        result = text\n        result = re.sub('[0-9]{5,}', '#####', result)\n        result = re.sub('[0-9]{4}', '####', result)\n        result = re.sub('[0-9]{3}', '###', result)\n        result = re.sub('[0-9]{2}', '##', result)\n        return result\n\n    def clean_misspelling(self, text: list):\n        mispell_dict = {'colour':'color',\n                'centre':'center',\n                'didnt':'did not',\n                'doesnt':'does not',\n                'isnt':'is not',\n                'shouldnt':'should not',\n                'favourite':'favorite',\n                'travelling':'traveling',\n                'counselling':'counseling',\n                'theatre':'theater',\n                'cancelled':'canceled',\n                'labour':'labor',\n                'organisation':'organization',\n                'wwii':'world war 2',\n                'citicise':'criticize',\n                'instagram': 'social medium',\n                'whatsapp': 'social medium',\n                'snapchat': 'social medium'}\n\n        def _get_mispell(mispell_dict):\n            mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n            return mispell_dict, mispell_re\n\n        mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n        def replace(match):\n            return mispellings[match.group(0)]\n\n        return mispellings_re.sub(replace, text)\n    \n    def apply_cleaning_function(self, fn, texts: list, description = \"\"):\n        result = [fn(text) for text in tqdm(texts)]\n        sentences = [text.split() for text in result]\n        tf_dict = self.build_tf_dict(sentences)\n        oov = self.check_coverage(tf_dict)\n        print(oov[:10])\n\n        return result\n\n    def preprocess_for_embeddings_coverage(self, texts: list):\n        result = texts\n\n        sentences = [text.split() for text in result]\n        tf_dict = self.build_tf_dict(sentences)\n        oov = self.check_coverage(tf_dict)\n\n        result = self.apply_cleaning_function(lambda x: self.clean_punctuation(x), result, \"Cleaning punctuation...\")\n        result = self.apply_cleaning_function(lambda x: self.clean_digits(x), result, \"Cleaning numbers...\")\n        result = self.apply_cleaning_function(lambda x: self.clean_misspelling(x), result, \"Cleaning misspelled words...\")\n\n        return result",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b3581cef519ac859ddb5e7f3137a4d4f7e7b52a8"
      },
      "cell_type": "code",
      "source": "from gensim.models import KeyedVectors\n\nembedding_files = [\n    \"../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\",\n    \"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\",\n    \"../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec\",\n    \"../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt\"]\n    \nload_embedding_functions = [ #\n    lambda: KeyedVectors.load_word2vec_format(embedding_files[0], binary=True),\n    lambda: load_embeddings(embedding_files[1]),\n    lambda: load_embeddings(embedding_files[2]),\n    lambda: load_embeddings(embedding_files[3])]\n\nfor index, load_embeddings_fn in enumerate(load_embedding_functions):\n    print(f\"Training with {embedding_files[index]}\")\n    print(f\"==============================================================================================================\")\n    embeddings = load_embeddings_fn()\n    preprocessor = Preprocessor(embeddings)\n    preprocessor.preprocess_for_embeddings_coverage(question_texts)",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Training with ../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\n==============================================================================================================\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8b9dc2afde800ca12bbb555027f20e1552c78345"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}